{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBqubaUqATYg"
   },
   "source": [
    "First, please install the packages needed for the conversational AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install evaluate\n",
    "! pip install torch\n",
    "! pip install transformers\n",
    "### maybe you will need to install some additional packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wEE-KgBF37n"
   },
   "source": [
    "# 1 Conversational question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbb0ItNfF_X6"
   },
   "source": [
    "### 1.1 You want to build a dense passage retriever for the question-answering task. Now you have the labeled question-answering dataset, and each sample contains the question, answer, and related passages. From the lecture, you know that we can build the negative passages by using any random passage from the dataset. Do you have any other idea to build the negative passages such that fine-tuned model gets better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sudwxCRnpVQ0"
   },
   "source": [
    "### 1.2 After building the training dataset for the dense passage retriever, we have B questions in a mini-batch, and each one is associated with a relevant passage. Do you have any idea to make the training effective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALYwRZTmpX2r"
   },
   "source": [
    "### 1.3 From the lecture, we know that the system should consider the dialogue history for the conversational question-answering task. Now we have the following conversational question-answering task and want to use a GPT2 model to generate the correct answer. How will you design the input for the GPT2 model?\n",
    "\n",
    "Jessica went to sit in her rocking chair. Today was her birthday and she was turning 80. Her granddaughter Annie was coming over in the afternoon and Jessica was very excited to see her. Her daughter Melanie and Melanie’s husband Josh were coming as well. Jessica had …\n",
    "\n",
    "Q1: Who had a birthday?\n",
    "A1: Jessica\n",
    "Q2: How old would she be?\n",
    "A2: 80\n",
    "Q3: Did she plan to have any visitors?\n",
    "A3: Yes\n",
    "Q4: How many?\n",
    "A4: Three\n",
    "Q5: Who?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLAs3SyaZ-En"
   },
   "source": [
    "## 1.4 Dense passage retriever from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "\n",
    "####################################################################\n",
    "    # TODO choose the suitable question model and tokenizer for question from huggingface\n",
    "####################################################################\n",
    "tokenizer_question = DPRQuestionEncoderTokenizer.from_pretrained(\"\")\n",
    "model_question = DPRQuestionEncoder.from_pretrained(\"\")\n",
    "####################################################################\n",
    "    # TODO Tokenize the question and feed the tokenizer \n",
    "    # output into the question model.\n",
    "    # question \"what is ChatGPT ?\"\n",
    "####################################################################\n",
    "print(embeddings_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "####################################################################\n",
    "    # TODO choose the suitable context model and tokenizer for context from huggingface\n",
    "####################################################################\n",
    "tokenizer_context = DPRContextEncoderTokenizer.from_pretrained(\"\")\n",
    "model_context = DPRContextEncoder.from_pretrained(\"\")\n",
    "####################################################################\n",
    "    # TODO Tokenize the passage 1 and feed the tokenizer \n",
    "    # output into the context model.\n",
    "    # passage 1 \"ChatGPT is an artificial intelligence chatbot developed by OpenAI.\"\n",
    "####################################################################\n",
    "print(embeddings_p1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "####################################################################\n",
    "    # TODO calculate the score between the user query and the passage 1\n",
    "####################################################################\n",
    "print('The score between the user query and the passage 1 is: ', score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "####################################################################\n",
    "    # TODO Tokenize the passage 2 and feed the tokenizer \n",
    "    # output into the context model.\n",
    "    # passage 2 \"A chatbot amis to simulate human communication.\"\n",
    "####################################################################\n",
    "print(embeddings_p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "    # TODO calculate the score between the user query and the passage 2\n",
    "####################################################################\n",
    "print('The score between the user query and the passage 2 is: ', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0CRI_s3Ycqp"
   },
   "source": [
    "## 1.5 DualEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu8VV_JaoCLH"
   },
   "source": [
    "We can also only use the query encoder to do the retrieval. What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "####################################################################\n",
    "    # TODO choose the suitable question model and tokenizer for question from huggingface\n",
    "####################################################################\n",
    "tokenizer_question = DPRQuestionEncoderTokenizer.from_pretrained(\"\")\n",
    "model_question = DPRQuestionEncoder.from_pretrained(\"\")\n",
    "####################################################################\n",
    "    # TODO Tokenize the question and feed the tokenizer \n",
    "    # output into the question model.\n",
    "    # question \"what is ChatGPT ?\"\n",
    "####################################################################\n",
    "print(embeddings_q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "####################################################################\n",
    "    # TODO Tokenize the passage 1 and feed the tokenizer \n",
    "    # output into the question model.\n",
    "    # passage 1 \"ChatGPT is an artificial intelligence chatbot developed by OpenAI.\"\n",
    "####################################################################\n",
    "print(embeddings_p1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "####################################################################\n",
    "    # TODO calculate the score between the user query and the passage 1\n",
    "####################################################################\n",
    "print('The score between the user query and the passage 1 is: ', score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "####################################################################\n",
    "    # TODO Tokenize the passage 2 and feed the tokenizer \n",
    "    # output into the question model.\n",
    "    # passage 2 \"A chatbot amis to simulate human communication.\"\n",
    "####################################################################\n",
    "print(embeddings_p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "####################################################################\n",
    "    # TODO calculate the score between the user query and the passage 2\n",
    "####################################################################\n",
    "print('The score between the user query and the passage 2 is: ', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJsfh-d1aSTl"
   },
   "source": [
    "# 2 Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS9NJHTzKnmC"
   },
   "source": [
    "## 2.1 Open [ChatGPT](https://openai.com/blog/chatgpt) and explore some questions that ChatGPT will make the hallucination problem. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9vz_XuE6O7v"
   },
   "source": [
    "## 2.2 Personal Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hf_09JyvsTYM"
   },
   "source": [
    "At first, load the models and their tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"af1tang/personaGPT\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"af1tang/personaGPT\")\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "## utility functions ##\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 33933,
     "status": "ok",
     "timestamp": 1683104948814,
     "user": {
      "displayName": "Miri Ull",
      "userId": "07195528894595547821"
     },
     "user_tz": -120
    },
    "id": "HShIye7MyYim"
   },
   "outputs": [],
   "source": [
    "def to_data(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.Tensor(x)\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dialog_history(dialog_hx):\n",
    "    for j, line in enumerate(dialog_hx):\n",
    "        msg = tokenizer.decode(line)\n",
    "        if j %2 == 0:\n",
    "            print(\">> User: \"+ msg)\n",
    "        else:\n",
    "            print(\"Bot: \"+msg)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next(bot_input_ids, do_sample=True, top_k=10, top_p=.92,\n",
    "                  max_length=1000, pad_token=tokenizer.eos_token_id):\n",
    "    full_msg = model.generate(bot_input_ids, do_sample=True,\n",
    "                                              top_k=top_k, top_p=top_p, \n",
    "                                              max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
    "    msg = to_data(full_msg.detach()[0])[bot_input_ids.shape[-1]:]\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECxzAntzsXw3"
   },
   "source": [
    "Define personality of the Chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personas = []\n",
    "for i in range(3):\n",
    "    response = input(\">> Fact %d: \"%(i+1))+ tokenizer.eos_token\n",
    "    personas.append(response)\n",
    "personas = tokenizer.encode(''.join(['<|p2|>'] + personas + ['<|sep|>'] + ['<|start|>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMNIcSKM4xZp"
   },
   "source": [
    "## 2.2.1 Try to talk with the defined personal chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converse for 8 turns\n",
    "dialog_hx = []\n",
    "for step in range(4):\n",
    "    # encode the user input\n",
    "    user_inp = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token)\n",
    "    # append to the chat history\n",
    "    dialog_hx.append(user_inp)\n",
    "        \n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    bot_input_ids = to_var([personas + flatten(dialog_hx)]).long()\n",
    "    msg = generate_next(bot_input_ids)\n",
    "    dialog_hx.append(msg)\n",
    "    print(\"Bot: {}\".format(tokenizer.decode(msg, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuiKNvt8WUBc"
   },
   "source": [
    "### 2.2.2 Define the action space of the personal chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## available actions ##\n",
    "####################################################################\n",
    "    # TODO add ten actions into the action_space\n",
    "    # Define the top_k, top_p, max_lenth by yourself\n",
    "####################################################################\n",
    "action_space = [ 'ask about age.', ]\n",
    "# converse for 8 turns\n",
    "dialog_hx = []\n",
    "for step in range(4):\n",
    "    # choose an action\n",
    "    act = None\n",
    "    while act not in action_space:\n",
    "        display_dialog_history(dialog_hx)\n",
    "        print()\n",
    "        print(\" actions: \")\n",
    "        for k,v in enumerate(action_space): print(k,v)\n",
    "        try:\n",
    "            act = action_space[int(input(\" input [0-10]: \" ))]\n",
    "        except:\n",
    "            act = None\n",
    "    print()\n",
    "    # format into prefix code\n",
    "    action_prefix = tokenizer.encode(''.join(['<|act|> '] + [act] + ['<|p1|>'] + [] + ['<|sep|>'] + ['<|start|>']))\n",
    "    bot_input_ids = to_var([action_prefix + flatten(dialog_hx)]).long()\n",
    "    \n",
    "    # generate query conditioned on action\n",
    "    msg = generate_next(bot_input_ids, top_k=top_k, top_p=top_p, max_length=max_length)\n",
    "    dialog_hx.append(msg)\n",
    "    \n",
    "    # generate bot response\n",
    "    bot_input_ids = to_var([personas+ flatten(dialog_hx)]).long()\n",
    "    msg = generate_next(bot_input_ids, top_k=top_k, top_p=top_p, max_length=max_length)\n",
    "    dialog_hx.append(msg)\n",
    "display_dialog_history(dialog_hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umMIODe64eqF"
   },
   "source": [
    "## 3 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wrg8s8F9nlNA"
   },
   "source": [
    "### MWOZ data \n",
    "There is an example of the MWOZ dataset. \n",
    "\n",
    "User: I'm looking for an expensive restaurant in the centre.\n",
    "\n",
    "System: I can recommend several restaurants in the centre of town. Are you looking for any type of food in particular.\n",
    "\n",
    "User: Yes, Caribbean food please.\n",
    "\n",
    "System: There is no matching restaurant. Would you like to change your search criteria?\n",
    "\n",
    "User: What about a restaurant that serves european food?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Please build the dialogue state of the last user turn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 If we want to generate the system response by using SimpleToD , how can we build the input?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Now we use SimpleToD generate the following response. We also get the groundtruth from the labeled dataset. Can you evaluate them using automatic evaluation metric bleu and write the code for it?\n",
    "generated sentence: There is 1 expensive modern european restaurant. Do you want me to book it for you?\n",
    "\n",
    "groundtruth: I found 2 expensive european restaurants and 1 expensive modern european restaurant. Which kind would you prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434,
     "referenced_widgets": [
      "c1e3bcc8528f4372a0d471082dd487f3",
      "232342dad1ed47598ecda51520b6e03e",
      "5644b371819e483bb34a9b0786cb8472",
      "e73cd6a13667430e9c0750466a6b3f6d",
      "bcde47a2f6dd4c49a788deade2cdf2bb",
      "4a498e5832e54ce89b58985a30535a59",
      "6b36097f9b7449faa527a5bb0716af5c",
      "fde2b0642b624295ba15de0960f4eaab",
      "3fe8f8907e81487a986fc254043f6306",
      "4fdf2149317544c488de7bfe1d6800ee",
      "96dbf2aa7c194eaa8dde9d39d436bde4"
     ]
    },
    "executionInfo": {
     "elapsed": 18833,
     "status": "ok",
     "timestamp": 1683103388026,
     "user": {
      "displayName": "Miri Ull",
      "userId": "07195528894595547821"
     },
     "user_tz": -120
    },
    "id": "nQwUGG714eau",
    "outputId": "fea60470-bb3a-44a5-ea29-739692e75bb8"
   },
   "outputs": [],
   "source": [
    "from evaluate import load # use the Huggingface evaluate implementations\n",
    "bleu = load(\"sacrebleu\")\n",
    "####################################################################\n",
    "# TODO \n",
    "# Finish the code part\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Now we want to do the human evaluation for the generated response. Could you please design the questions for the human evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., ... & Yih, W. T. (2020, November). Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 6769-6781).\n",
    "\n",
    "Huggingface https://huggingface.co/ DPR model, PersonaGPT\n",
    "\n",
    "ChatGPT Introducing ChatGPT https://openai.com/blog/chatgpt\n",
    "\n",
    "Tang, F., Zeng, L., Wang, F., & Zhou, J. (2021). Persona authentication through generative dialogue. arXiv preprint arXiv:2110.12949.\n",
    "\n",
    "Radford, Alec, et al. \"Language models are unsupervised multitask learners.\" OpenAI Blog 1.8 (2019): 9.\n",
    "\n",
    "Zhang, Yizhe, et al. \"Dialogpt: Large-scale generative pre-training for conversational response generation.\" arXiv preprint arXiv:1911.00536 (2019).\n",
    "\n",
    "Zhang, Saizheng, et al. \"Personalizing dialogue agents: I have a dog, do you have pets too?.\" arXiv preprint arXiv:1801.07243 (2018).\n",
    "\n",
    "Dinan et al., \"The Second Conversational Intelligence Challenge (ConvAI2).\" arXiv preprint arXiv:1902.00098 (2019).\n",
    "\n",
    "Thomas Wolf et al. \"Transfertransfo: A transfer learning approach for neural network based conversational agents.\" arXiv preprint328arXiv:1901.08149, 2019\n",
    "\n",
    "MWOZ dataset https://github.com/budzianowski/multiwoz\n",
    "\n",
    "Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Gašić. 2018. MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016–5026, Brussels, Belgium. Association for Computational Linguistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAAI",
   "language": "python",
   "name": "caai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
