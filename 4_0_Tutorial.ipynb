{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3vguvi7izX6"
   },
   "source": [
    "# 0 Installing Packages Needed in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbgXNLjeizX9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade tensorflow_datasets\n",
    "!pip install --upgrade tensorflow\n",
    "!pip install --upgrade nltk\n",
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade rouge\n",
    "!pip install --upgrade sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FO3wVkjJizX_"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W085eQ0IizX_"
   },
   "source": [
    "# 1 Load and preview dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD_RIW_nizYA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTsvdYhgizYA"
   },
   "source": [
    "#### Load dataset via tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnKFBUtyizYA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = tfds.load('cnn_dailymail', split=\"test[:5%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDExJCMeizYB"
   },
   "source": [
    "#### Extract the documents and summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTDF7kTNizYB"
   },
   "outputs": [],
   "source": [
    "# Step 2: Preprocess\n",
    "documents = [example['article'].numpy().decode() for example in dataset]\n",
    "summaries = [example['highlights'].numpy().decode() for example in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEHQVQvVizYB"
   },
   "source": [
    "#### Preview the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Id3PsABXizYC"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for i in range(3):\n",
    "    index = random.randint(0, len(documents))\n",
    "    print(f\"Document: \\n--------\\n{documents[index]}\")\n",
    "    print(f\"Summary: \\n--------\\n{summaries[index]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gf_zZaPizYC"
   },
   "source": [
    "# Extractive Summarization Exercise with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wU4sqr1izYD"
   },
   "source": [
    "#### Import the necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEl0-R55izYD"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HpCnxVtizYD"
   },
   "source": [
    "#### Implement TF-IDF for extractive summarization at sentence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COHeOYKQizYE"
   },
   "outputs": [],
   "source": [
    "def tfidf_extractive_summarization(document: str, top_n: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    #TODO: Implement a TFIDF Extractive Summarization approach\n",
    "    Method should receive the document and receive the top sentences concatenated.\n",
    "\n",
    "    Hint, follow the following steps:\n",
    "    # Tokenize the document into sentences\n",
    "    # Create a TF-IDF vectorizer\n",
    "    # Fit and transform the sentences\n",
    "    # Calculate the sentence scores\n",
    "    # Get the indices of the top N sentences with the highest scores\n",
    "    # Sort the top sentences based on their original order in the document\n",
    "    # Join the top sentences to form the summary\n",
    "    \"\"\"\n",
    "    # TODO Provide your implementation here\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRY5ZK4BizYE"
   },
   "source": [
    "#### Select an example document for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4w8YyMyizYE"
   },
   "outputs": [],
   "source": [
    "example_index = random.randint(0, len(documents))  # Index of the example you want to summarize\n",
    "document = documents[example_index]\n",
    "summary = summaries[example_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfTtqK4LizYF"
   },
   "source": [
    "#### Generate the summaries using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vj47a7S1izYF"
   },
   "outputs": [],
   "source": [
    "tfidf_summary = tfidf_extractive_summarization(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puKWATKvizYF"
   },
   "source": [
    "#### Print the original document and the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M98F0XpBizYG"
   },
   "outputs": [],
   "source": [
    "print(\"Original Document:\")\n",
    "print(document)\n",
    "print()\n",
    "print(\"Reference Summary:\")\n",
    "print(summary)\n",
    "print()\n",
    "print(\"TF-IDF Summary:\")\n",
    "print(tfidf_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23RmVfReizYG"
   },
   "source": [
    "# 2 Metrics Evaluation for Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qv3-ymk5izYG"
   },
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ws2xUkPizYH"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJw5cDPrizYH"
   },
   "source": [
    "#### Load the reference summaries and the generated summaries for TFIDF and BERTSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCJ9AeoFizYH"
   },
   "outputs": [],
   "source": [
    "documents_number = 3\n",
    "\n",
    "random_indices = [random.randint(0, len(documents)) for i in range(documents_number)]\n",
    "\n",
    "reference_summaries = [\n",
    "    summaries[index] for index in random_indices\n",
    "]\n",
    "\n",
    "original_documents = [\n",
    "    documents[index] for index in random_indices\n",
    "]\n",
    "\n",
    "tf_idf_generated_summaries = [\n",
    "    tfidf_extractive_summarization(document) for document in original_documents\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHjsgDDqizYH"
   },
   "source": [
    "#### Calculate ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76EuaBcbizYI"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "def calculate_rouge_scores(reference_summaries, generated_summaries) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    :param reference_summaries: list of reference summaries for evaluation\n",
    "    :param generated_summaries: list of generated summaries for evalutation\n",
    "\n",
    "    # TODO implement a method for calculating the ROUGE-1, ROUGE-2, ROUGE-L\n",
    "    Method should return two lists, first as a list of [rouge-1, rouge-2] per sentence, second as a list of rouge-l for the sentences.\n",
    "\n",
    "    \"\"\"\n",
    "    rouge = Rouge()\n",
    "    rouge_n_scores = []\n",
    "    rouge_l_scores = []\n",
    "\n",
    "    for reference, generated in zip(reference_summaries, generated_summaries):\n",
    "        # TODO generate the rouge scores for the sentences\n",
    "\n",
    "\n",
    "    return rouge_n_scores, rouge_l_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "co07vF3jizYI"
   },
   "outputs": [],
   "source": [
    "rouge_n_scores, rouge_l_scores = calculate_rouge_scores(reference_summaries, tf_idf_generated_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs-bF1SiizYI"
   },
   "source": [
    "#### Print ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcitEFPvizYJ"
   },
   "outputs": [],
   "source": [
    "print(\"ROUGE-N Scores:\")\n",
    "for n, scores in enumerate(rouge_n_scores, start=1):\n",
    "    print(f\"ROUGE-Summary-{n}: {scores}\")\n",
    "print()\n",
    "\n",
    "print(\"ROUGE-L Scores:\")\n",
    "for i, score in enumerate(rouge_l_scores, start=1):\n",
    "    print(f\"ROUGE-L-Summary-{i}: {score}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBeYdlG4izYK"
   },
   "source": [
    "#### Calculate BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOJ9F2rMizYK"
   },
   "outputs": [],
   "source": [
    "def calculate_bleu_score(reference_summaries: List, generated_summaries: List) -> List:\n",
    "    \"\"\"\n",
    "    :param reference_summaries: list of reference summaries for evaluation\n",
    "    :param generated_summaries: list of generated summaries for evalutation\n",
    "\n",
    "    #TODO Implement BLEU for evaluation\n",
    "    Hint: add a smoothing function for 0 n-grams matching, `SmoothingFunction().method1`\n",
    "    Hint 2: sentence bleu receives tokens splitted.\n",
    "    \"\"\"\n",
    "    bleu_scores = []\n",
    "\n",
    "    for reference, generated in zip(reference_summaries, generated_summaries):\n",
    "        #TODO Implement BLEU evaluation here\n",
    "\n",
    "    return bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVCrpbBBizYL"
   },
   "outputs": [],
   "source": [
    "bleu_scores = calculate_bleu_score(reference_summaries, tf_idf_generated_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSIRatRSizYL"
   },
   "outputs": [],
   "source": [
    "print(\"BLEU Scores:\")\n",
    "for i, score in enumerate(bleu_scores, start=1):\n",
    "    print(f\"BLEU-Sentence-{i}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLF2QNgXizYM"
   },
   "source": [
    "#### Calculate METEOR score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMxHc0UYizYM"
   },
   "outputs": [],
   "source": [
    "def calculate_meteor_score(reference_summaries, generated_summaries):\n",
    "    \"\"\"\n",
    "    :param reference_summaries: list of reference summaries for evaluation\n",
    "    :param generated_summaries: list of generated summaries for evalutation\n",
    "\n",
    "    #TODO Implement METEOR for evaluation\n",
    "    Hint: `meteor_score` receives two arguments, first a list of reference summaries' tokens (List[List[str]]),\n",
    "    second list of generated summaries tokens\n",
    "    \"\"\"\n",
    "    meteor_scores = []\n",
    "\n",
    "    for reference, generated in zip(reference_summaries, generated_summaries):\n",
    "        # TODO Provide your implementation here.\n",
    "\n",
    "    return meteor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QajflwMaizYM"
   },
   "outputs": [],
   "source": [
    "meteor_scores = calculate_meteor_score(reference_summaries, tf_idf_generated_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQTZsf0NizYN"
   },
   "outputs": [],
   "source": [
    "print(\"METEOR Scores:\")\n",
    "for i, score in enumerate(meteor_scores, start=1):\n",
    "    print(f\"METEOR-Sentence-{i}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn3hT-WuizYN"
   },
   "source": [
    "# 3 Abstractive Summmarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmFA9zndizYO"
   },
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nDTlGKUwizYO"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, BartTokenizer, PegasusTokenizer\n",
    "from transformers import AutoTokenizer, PegasusForConditionalGeneration, pipeline, ProphetNetForConditionalGeneration, ProphetNetTokenizer\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpH-pWMtizYP"
   },
   "source": [
    "#### Implement BART for abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MuFAy2VizYP"
   },
   "outputs": [],
   "source": [
    "def bart_abstractive_summarization(document: str) -> str:\n",
    "    \"\"\"\n",
    "    :param document: Document to summarize\n",
    "    # TODO Implement a BART pipeline for text summarization\n",
    "    # hint: use `bart-large-cnn`\n",
    "    \"\"\"\n",
    "    # TODO Provide your implementation here\n",
    "\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzIEvR3dizYP"
   },
   "source": [
    "#### Select an example document for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw0u5uLFizYP"
   },
   "outputs": [],
   "source": [
    "document = documents[example_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7QYxuqQizYQ"
   },
   "source": [
    "#### Implement Pegasus for abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDazWLL5izYQ"
   },
   "outputs": [],
   "source": [
    "def pegasus_abstractive_summarization(document):\n",
    "    \"\"\"\n",
    "    :param document: Document to summarize\n",
    "    # TODO Implement a PEGASUS pipeline for text summarization\n",
    "    # hint: use `google/pegasus-cnn_dailymail` from hugging face\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: provide your implementation here\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJfdoO-jizYR"
   },
   "source": [
    "#### Generate the summaries using BART and Pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCb5wuJ9izYR"
   },
   "outputs": [],
   "source": [
    "pegasus_summary = pegasus_abstractive_summarization(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYgZ4RbjizYR"
   },
   "outputs": [],
   "source": [
    "bart_summary = bart_abstractive_summarization(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3fOnBlDizYR"
   },
   "source": [
    "#### Print the original document and the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgvdO9paizYS"
   },
   "outputs": [],
   "source": [
    "# Step 7:\n",
    "print(\"Original Document:\")\n",
    "print(document)\n",
    "print()\n",
    "print(\"Reference Summary:\")\n",
    "print(summary)\n",
    "print()\n",
    "print(\"BART Summary:\")\n",
    "print(bart_summary)\n",
    "print()\n",
    "print(\"Pegasus Summary:\")\n",
    "print(pegasus_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDV5-mNrizYS"
   },
   "source": [
    "#### Calculate metrics for the generated summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_mNgzplONM0"
   },
   "outputs": [],
   "source": [
    "bart_summaries = [\n",
    "    bart_abstractive_summarization(document) for document in original_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfizPOTfizYS"
   },
   "outputs": [],
   "source": [
    "rouge_n_scores, rouge_l_scores = calculate_rouge_scores(reference_summaries, bart_summaries)\n",
    "bleu_scores = calculate_bleu_score(reference_summaries, bart_summaries)\n",
    "meteor_scores = calculate_meteor_score(reference_summaries, bart_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZvtsCaNNinE"
   },
   "outputs": [],
   "source": [
    "print(\"ROUGE-N Scores:\")\n",
    "for n, scores in enumerate(rouge_n_scores, start=1):\n",
    "    print(f\"ROUGE-Summary-{n}: {scores}\")\n",
    "print()\n",
    "\n",
    "print(\"ROUGE-L Scores:\")\n",
    "for i, score in enumerate(rouge_l_scores, start=1):\n",
    "    print(f\"ROUGE-L-Summary-{i}: {score}\")\n",
    "print()\n",
    "print(\"BLEU Scores:\")\n",
    "for i, score in enumerate(bleu_scores, start=1):\n",
    "    print(f\"BLEU-Sentence-{i}: {score}\")\n",
    "print()\n",
    "print(\"METEOR Scores:\")\n",
    "for i, score in enumerate(meteor_scores, start=1):\n",
    "    print(f\"METEOR-Sentence-{i}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWSwJcheQJP7"
   },
   "outputs": [],
   "source": [
    "pegasus_summaries = [\n",
    "    pegasus_abstractive_summarization(document) for document in original_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40JpgOnHu8pb"
   },
   "outputs": [],
   "source": [
    "rouge_n_scores, rouge_l_scores = calculate_rouge_scores(reference_summaries, pegasus_summaries)\n",
    "bleu_scores = calculate_bleu_score(reference_summaries, pegasus_summaries)\n",
    "meteor_scores = calculate_meteor_score(reference_summaries, pegasus_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhIdiyGRRgIy"
   },
   "outputs": [],
   "source": [
    "print(\"ROUGE-N Scores:\")\n",
    "for n, scores in enumerate(rouge_n_scores, start=1):\n",
    "    print(f\"ROUGE-Summary-{n}: {scores}\")\n",
    "print()\n",
    "\n",
    "print(\"ROUGE-L Scores:\")\n",
    "for i, score in enumerate(rouge_l_scores, start=1):\n",
    "    print(f\"ROUGE-L-Summary-{i}: {score}\")\n",
    "print()\n",
    "print(\"BLEU Scores:\")\n",
    "for i, score in enumerate(bleu_scores, start=1):\n",
    "    print(f\"BLEU-Sentence-{i}: {score}\")\n",
    "print(\"METEOR Scores:\")\n",
    "for i, score in enumerate(meteor_scores, start=1):\n",
    "    print(f\"METEOR-Sentence-{i}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kg7dctiLizYS"
   },
   "outputs": [],
   "source": [
    "def prophetnet_abstractive_summarization(document: str) -> str:\n",
    "    \"\"\"\n",
    "    :param document: Document to summarize\n",
    "    # TODO Implement a PophetNET pipeline for text summarization\n",
    "    # hint: use `microsoft/prophetnet-large-uncased-cnndm`\n",
    "    \"\"\"\n",
    "    # TODO Provide your implementation here\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NDZMOAzRizYT"
   },
   "outputs": [],
   "source": [
    "prophetnet_summary = prophetnet_abstractive_summarization(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovlfhvqQvo7r"
   },
   "outputs": [],
   "source": [
    "prophetnet_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciWYbvRDR2T_"
   },
   "outputs": [],
   "source": [
    "prophetnet_summaries = [\n",
    "    prophetnet_abstractive_summarization(document) for document in original_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toWqJbbxR5Mh"
   },
   "outputs": [],
   "source": [
    "rouge_n_scores, rouge_l_scores = calculate_rouge_scores(reference_summaries, prophetnet_summaries)\n",
    "bleu_scores = calculate_bleu_score(reference_summaries, prophetnet_summaries)\n",
    "meteor_scores = calculate_meteor_score(reference_summaries, prophetnet_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sY_5PTMSA2p"
   },
   "outputs": [],
   "source": [
    "print(\"ROUGE-N Scores:\")\n",
    "for n, scores in enumerate(rouge_n_scores, start=1):\n",
    "    print(f\"ROUGE-Summary-{n}: {scores}\")\n",
    "print()\n",
    "\n",
    "print(\"ROUGE-L Scores:\")\n",
    "for i, score in enumerate(rouge_l_scores, start=1):\n",
    "    print(f\"ROUGE-L-Summary-{i}: {score}\")\n",
    "print()\n",
    "print(\"BLEU Scores:\")\n",
    "for i, score in enumerate(bleu_scores, start=1):\n",
    "    print(f\"BLEU-Sentence-{i}: {score}\")\n",
    "print()\n",
    "print(\"METEOR Scores:\")\n",
    "for i, score in enumerate(meteor_scores, start=1):\n",
    "    print(f\"METEOR-Sentence-{i}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U23lH23mlXPq"
   },
   "outputs": [],
   "source": [
    "# Step 4: Implement T5 for abstractive summarization\n",
    "def t5_abstractive_summarization(document: str) -> str:\n",
    "    \"\"\"\n",
    "    :param document: Document to summarize\n",
    "    # TODO Implement a T5 pipeline for text summarization\n",
    "    # hint: use `T5-large`\n",
    "    \"\"\"\n",
    "    #TODO Provide your implementation here\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDaxQc_ClZEK"
   },
   "outputs": [],
   "source": [
    "t5_summary = t5_abstractive_summarization(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7qigk2zHFDr"
   },
   "outputs": [],
   "source": [
    "t5_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48bjDi0YizYT"
   },
   "outputs": [],
   "source": [
    "print(\"Original Document:\")\n",
    "print(document)\n",
    "print()\n",
    "print(\"Reference Summary:\")\n",
    "print(summary)\n",
    "print()\n",
    "print(\"ProphetNET Summary:\")\n",
    "print(prophetnet_summary)\n",
    "print()\n",
    "print(\"T5 Summary:\")\n",
    "print(t5_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpzRmyY8Tc9P"
   },
   "outputs": [],
   "source": [
    "t5_summaries = [\n",
    "    t5_abstractive_summarization(document) for document in original_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCXkyQI6T-9T"
   },
   "outputs": [],
   "source": [
    "rouge_n_scores, rouge_l_scores = calculate_rouge_scores(reference_summaries, t5_summaries)\n",
    "bleu_scores = calculate_bleu_score(reference_summaries, t5_summaries)\n",
    "meteor_scores = calculate_meteor_score(reference_summaries, t5_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrVwCf00Th0N"
   },
   "outputs": [],
   "source": [
    "print(\"ROUGE-N Scores:\")\n",
    "for n, scores in enumerate(rouge_n_scores, start=1):\n",
    "    print(f\"ROUGE-Summary-{n}: {scores}\")\n",
    "print()\n",
    "\n",
    "print(\"ROUGE-L Scores:\")\n",
    "for i, score in enumerate(rouge_l_scores, start=1):\n",
    "    print(f\"ROUGE-L-Summary-{i}: {score}\")\n",
    "print()\n",
    "print(\"BLEU Scores:\")\n",
    "for i, score in enumerate(bleu_scores, start=1):\n",
    "    print(f\"BLEU-Sentence-{i}: {score}\")\n",
    "print()\n",
    "print(\"METEOR Scores:\")\n",
    "for i, score in enumerate(meteor_scores, start=1):\n",
    "    print(f\"METEOR-Sentence-{i}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5Cld8RciG5l"
   },
   "source": [
    "# Concluding Discussions\n",
    "\n",
    "* According to the calculated metrics and the implemented models; draw your conculsion with respect to the different model's output, quality, and metrics.\n",
    "* Discuss approaches which could improve the model performance\n",
    "* Experiment with different models, more dataset, bigger batch from the CNN/Daily mail test data\n",
    "* Experiment with different truncation approaches for the models"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
